{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA and knowledge triple in ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import jsonlines\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "def file_reader(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        src = []\n",
    "        tgt = []\n",
    "        for obj in reader:\n",
    "            src.append(obj['src'])\n",
    "            tgt.append(obj['tgt'])\n",
    "    return src, tgt\n",
    "\n",
    "def get_query(input):\n",
    "    query = []\n",
    "    src = []\n",
    "    for item in input:\n",
    "        temp = item.split('</s>')\n",
    "        query.append(temp[0].replace('<s>','').strip())\n",
    "        src.append(temp[1].replace('<s>','').strip())\n",
    "    return query, src\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input['input_ids'] = encoded_input['input_ids'].cuda()\n",
    "    encoded_input['token_type_ids'] = encoded_input['token_type_ids'].cuda()\n",
    "    encoded_input['attention_mask'] = encoded_input['attention_mask'].cuda()\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "# model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\").cuda()\n",
    "from transformers.models.bart.modeling_bart import shift_tokens_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/281 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8ee61d9ae852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#Encode query and docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mquery_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mdoc_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#Compute dot score between query and all document embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-d85a1647ad2e>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mencoded_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mencoded_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mencoded_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Compute token embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3090/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "src, tgt = file_reader('processed_data_no_sent_split/test.jsonl')\n",
    "querys, src = get_query(src)\n",
    "max_input_len = 512\n",
    "number_of_segment = 16\n",
    "src = [sent_tokenize(i) for i in src]\n",
    "segmented_input = []\n",
    "non_segmented_input = []\n",
    "\n",
    "for i in tqdm(range(len(src))):\n",
    "    seg_counter = 0\n",
    "    doc = src[i]\n",
    "    query = querys[i]\n",
    "    this_doc = []\n",
    "    this_seg = query + '</s></s>'\n",
    "    counter = len(tokenizer.tokenize(this_seg,add_special_tokens=False))\n",
    "    for sent in doc:\n",
    "        length = len(tokenizer.tokenize(sent,add_special_tokens=False))\n",
    "        if counter + length < max_input_len:\n",
    "            this_seg = this_seg + ' ' + sent\n",
    "            counter += length\n",
    "        else:\n",
    "            this_doc.append(this_seg.split('</s></s>')[-1])\n",
    "            this_seg = query + '</s></s>'+ sent\n",
    "            counter = len(tokenizer.tokenize(this_seg, add_special_tokens=False))\n",
    "    if len(tokenizer.tokenize(this_seg, add_special_tokens=False)) > 256:\n",
    "        this_doc.append(this_seg.split('</s></s>')[-1])\n",
    "\n",
    "    #Encode query and docs\n",
    "    query_emb = encode(query)\n",
    "    doc_emb = encode(this_doc)\n",
    "    #Compute dot score between query and all document embeddings\n",
    "    scores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n",
    "    #Combine docs & scores\n",
    "    doc_score_pairs = list(zip(this_doc, scores))\n",
    "    original_doc_score_pairs = copy.deepcopy(doc_score_pairs)\n",
    "    #Sort by decreasing score\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    if len(doc_score_pairs) <= number_of_segment:\n",
    "        segmented_input.append([query + '</s></s>' + doc for doc, score in doc_score_pairs] + ['' for i in range(number_of_segment - len(doc_score_pairs))])\n",
    "    else:\n",
    "        final_doc = []\n",
    "        for doc, score in doc_score_pairs:\n",
    "            if seg_counter >= number_of_segment:\n",
    "                break\n",
    "            final_doc.append(query + '</s></s>' + doc)\n",
    "            seg_counter += 1\n",
    "        segmented_input.append(final_doc)\n",
    "    non_segmented_input.append([query + '</s></s>' + doc for doc, score in original_doc_score_pairs])\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open( 'processed_data_no_sent_split/test_sort_16.pkl', 'wb') as f:\n",
    "#     pickle.dump(segmented_input, f)\n",
    "# with open( 'processed_data_no_sent_split/test_all_seg.pkl', 'wb') as f:\n",
    "#     pickle.dump(non_segmented_input, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 只用knowledge，并且重新排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openie import StanfordOpenIE\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "properties = {\n",
    "    'openie.affinity_probability_cap': 2 / 3,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarize meeting .\n",
      "13\n",
      "Starting server with command: java -Xmx8G -cp /home/tiezheng/.stanfordnlp_resources/stanford-corenlp-4.1.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-4ab125a500644666.props -preload openie\n",
      "{'subject': 'this', 'relation': 'is', 'object': 'our last meeting'} summarize meeting .\n",
      "{'subject': 'i', 'relation': 'go from', 'object': 'meeting'} summarize meeting .\n",
      "{'subject': 'i', 'relation': 'go from', 'object': 'previous meeting'} summarize meeting .\n",
      "{'subject': \"'s\", 'relation': 'see', 'object': 'minutes from last meeting'} summarize meeting .\n",
      "{'subject': \"'s\", 'relation': 'see', 'object': 'minutes from meeting'} summarize meeting .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': 'i', 'relation': 'have do for', 'object': 'last meeting'} summarize meeting .\n",
      "{'subject': 'i', 'relation': 'still have do for', 'object': 'last meeting'} summarize meeting .\n",
      "{'subject': 'i', 'relation': 'still have do for', 'object': 'meeting'} summarize meeting .\n",
      "{'subject': 'i', 'relation': 'have do for', 'object': 'meeting'} summarize meeting .\n",
      "project manager user interface introduce prototype remote control ?\n",
      "13\n",
      "{'subject': 'we', 'relation': \"'ll have\", 'object': 'prototype presentation'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'were talking', 'object': 'about trying incorporate into our prototype'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'trying', 'relation': 'incorporate into', 'object': 'our prototype'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'so were talking', 'object': 'about trying incorporate into our prototype'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'so were talking', 'object': 'about trying to incorporate into our prototype'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'so were talking', 'object': 'trying incorporate into our prototype'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'were talking', 'object': 'trying to incorporate into our prototype'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'were talking', 'object': 'about trying to incorporate into our prototype'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'were talking', 'object': 'trying incorporate into our prototype'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'so were talking', 'object': 'trying to incorporate into our prototype'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'looked for', 'object': 'coming with our own remote'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'looked for', 'object': 'coming up with our remote'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'looked for', 'object': 'coming with our remote'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'looked for', 'object': 'coming up with our own remote'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'also had', 'object': 'kid remote'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'had', 'object': 'kid remote'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'had', 'object': 'kid remote for simple idea'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'also had', 'object': 'kid remote for simple idea'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'had', 'object': 'kid remote for idea'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': 'also had', 'object': 'kid remote for idea'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'would', 'object': 'would almost identical to ipod'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'would', 'object': 'would almost identical to ipod'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'would', 'object': 'would almost identical'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'would', 'object': 'would almost identical'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'would', 'object': 'would identical to ipod'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'would', 'object': 'would identical to ipod'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'would', 'object': 'would identical'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'would', 'object': 'would identical'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'is', 'object': 'anything'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'is', 'object': 'anything'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'is', 'object': 'there anything'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'user interface', 'relation': 'is', 'object': 'there anything'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'we', 'relation': \"'ll do\", 'object': 'group evaluation of prototype'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'project manager'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'project manager'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'be lost', 'object': 'then user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'be lost', 'object': 'then user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'project manager', 'relation': 'is with', 'object': 'speech recognition'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'project manager', 'relation': 'is with', 'object': 'speech recognition'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'your remote', 'relation': 'also would alarm', 'object': 'you'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'your remote', 'relation': 'would alarm', 'object': 'you'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'your remote', 'relation': 'would alarm', 'object': 'if somebody stole'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'your remote', 'relation': 'would alarm', 'object': 'somebody stole'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'your remote', 'relation': 'also would alarm', 'object': 'if somebody stole'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'your remote', 'relation': 'also would alarm', 'object': 'somebody stole'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'many mo remote controls', 'relation': 'is with', 'object': 'industrial designer'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'many mo remote controls', 'relation': 'is with', 'object': 'industrial designer'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'just user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'just user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'one', 'relation': 'would', 'object': 'would whole thing project manager'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'one', 'relation': 'would', 'object': 'would whole thing project manager'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'one', 'relation': 'would', 'object': 'would thing project manager'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'one', 'relation': 'would', 'object': 'would thing project manager'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'seventeen', 'relation': 'divided by', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'seventeen', 'relation': 'divided by', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'just one mo single mould'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'just one mo mould'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'one mo mould'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'one mo single mould'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'is', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'it', 'relation': 'is', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': \"'s\", 'relation': 'take away', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': \"'s\", 'relation': 'take away', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'you', 'relation': 'there go', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'you', 'relation': 'there go', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'you', 'relation': 'go', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'you', 'relation': 'go', 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'project process', 'relation': 'satisfaction with', 'object': 'means'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'you', 'relation': 'did think of', 'object': 'our project process'} project manager user interface introduce prototype remote control ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:14,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': 'i', 'relation': \"'m\", 'object': 'project manager'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'i', 'relation': \"'m\", 'object': 'project manager'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'you', 'relation': \"'ve know\", 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'you', 'relation': \"'ve know\", 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'they', 'relation': \"'ve just know\", 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'they', 'relation': \"'ve just know\", 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'they', 'relation': \"'ve know\", 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'they', 'relation': \"'ve know\", 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'you', 'relation': \"'ve just know\", 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'you', 'relation': \"'ve just know\", 'object': 'user interface'} project manager user interface introduce prototype remote control ?\n",
      "{'subject': 'project', 'relation': 'is', 'object': 'evaluated'} project manager user interface introduce prototype remote control ?\n",
      "marketing design product evaluation ?\n",
      "13\n",
      "{'subject': 'we', 'relation': 'will do', 'object': 'evaluation'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': 'need', 'object': 'have under criteria for evaluation'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': 'need', 'object': 'to have under criteria for evaluation'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': \"'ll do\", 'object': 'evaluation'} marketing design product evaluation ?\n",
      "{'subject': 'it', 'relation': \"'s made out\", 'object': 'marketing'} marketing design product evaluation ?\n",
      "{'subject': 'you', 'relation': 'do have', 'object': 'marketing presentation'} marketing design product evaluation ?\n",
      "{'subject': 'you', 'relation': 'do have', 'object': 'marketing presentation for us'} marketing design product evaluation ?\n",
      "{'subject': 'me', 'relation': 'do', 'object': 'evaluation'} marketing design product evaluation ?\n",
      "{'subject': 'me', 'relation': 'do', 'object': 'evaluation of criteria'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': \"'ll do\", 'object': 'group evaluation of prototype'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': \"'ll do\", 'object': 'group evaluation'} marketing design product evaluation ?\n",
      "{'subject': 'i', 'relation': 'would say', 'object': 'marketing'} marketing design product evaluation ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'marketing'} marketing design product evaluation ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'marketing'} marketing design product evaluation ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'marketing'} marketing design product evaluation ?\n",
      "{'subject': 'it', 'relation': 'has', 'object': 'marketing'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': 'have', 'object': 'lcd display marketing'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': 'actually have', 'object': 'lcd display marketing'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': 'so actually have', 'object': 'lcd display marketing'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': 'so have', 'object': 'lcd display marketing'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': 'so remove', 'object': 'our marketing'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': 'remove', 'object': 'our marketing'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': 'just remove', 'object': 'our marketing'} marketing design product evaluation ?\n",
      "{'subject': 'we', 'relation': 'so just remove', 'object': 'our marketing'} marketing design product evaluation ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:20, 10.21s/it]\n"
     ]
    }
   ],
   "source": [
    "with open( 'processed_data_no_sent_split/train_all_seg.pkl', 'rb') as f:\n",
    "    segmented_input = pickle.load(f)\n",
    "\n",
    "with StanfordOpenIE(properties=properties) as client:\n",
    "    new_segmented_input = []\n",
    "    for number, one_segmented_input in tqdm(enumerate(segmented_input)):\n",
    "        query_without_stopwords = \" \".join([item for item in one_segmented_input[0].split('</s></s>')[0].split() if item not in stopwords.words()])\n",
    "        print(query_without_stopwords)\n",
    "        print(len(one_segmented_input))\n",
    "        this_doc_dic = {}\n",
    "        for i,item in enumerate(segmented_input[number]):\n",
    "            origin_item = copy.deepcopy(item)\n",
    "            item = item.split('</s></s>')[-1]\n",
    "            counter = 0\n",
    "            triples = []\n",
    "            for triple in client.annotate(item):\n",
    "                for word in triple['subject'].split():\n",
    "                    if word in query_without_stopwords and word not in stopwords.words():\n",
    "                        counter += 1\n",
    "                        print(triple, query_without_stopwords)\n",
    "                        triples.append(triple)\n",
    "                for word in triple['object'].split():\n",
    "                    if word in query_without_stopwords and word not in stopwords.words():\n",
    "                        counter += 1\n",
    "                        triples.append(triple)\n",
    "                        print(triple, query_without_stopwords)\n",
    "            this_doc_dic[origin_item] = [triples, counter]\n",
    "        new_segmented_input.append(this_doc_dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理重新排序的结果 (rerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple2text(triples):\n",
    "    output = \"\"\n",
    "    for triple in triples:\n",
    "        this_triple_text = 'subject: ' + triple['subject'] + ' relation: ' + triple['relation'] +  ' object: ' + triple['object'] + '</s>'\n",
    "        output += this_triple_text\n",
    "    return output\n",
    "    \n",
    "with open( 'processed_data_no_sent_split/val_all_seg_with_triple.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "new_seg_input = []\n",
    "triples = []\n",
    "number_of_segment = 16\n",
    "for doc in data:\n",
    "    new = {k: v for k, v in sorted(doc.items(), key=lambda item: item[1][1], reverse=True)}\n",
    "    new_seg_list = list(new.keys())\n",
    "    # print(new[new_seg_list[0]][0])\n",
    "    # print(triple2text(new[new_seg_list[0]][0]))\n",
    "    # break\n",
    "    if len(new_seg_list) < number_of_segment:\n",
    "        new_seg_input.append(new_seg_list + ['' for i in range(number_of_segment - len(new_seg_list))])\n",
    "        triples.append([triple2text(new[new_seg_list[i]][0]) for i in range(len(new_seg_list))] + ['' for i in range(number_of_segment - len(new_seg_list))])\n",
    "    else:\n",
    "        new_seg_input.append(new_seg_list[:number_of_segment])\n",
    "        triples.append([triple2text(new[new_seg_list[i]][0]) for i in range(number_of_segment)])\n",
    "\n",
    "\n",
    "with open('ka_rerank/val_triple.pkl', 'wb') as f:\n",
    "    pickle.dump(triples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( 'ka_rerank/val.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "518\n",
      "513\n",
      "516\n",
      "519\n",
      "515\n",
      "518\n",
      "516\n",
      "516\n",
      "525\n",
      "517\n",
      "513\n",
      "514\n",
      "513\n",
      "518\n",
      "513\n",
      "516\n",
      "517\n",
      "513\n",
      "514\n",
      "516\n",
      "517\n",
      "515\n",
      "516\n",
      "514\n",
      "515\n",
      "513\n",
      "514\n",
      "513\n",
      "515\n",
      "514\n",
      "513\n",
      "513\n",
      "513\n",
      "522\n",
      "522\n",
      "516\n",
      "513\n",
      "516\n",
      "518\n",
      "516\n",
      "514\n",
      "514\n",
      "514\n",
      "513\n",
      "516\n",
      "515\n",
      "516\n",
      "516\n",
      "513\n",
      "515\n",
      "519\n",
      "514\n",
      "517\n",
      "515\n",
      "517\n",
      "516\n",
      "513\n",
      "526\n",
      "514\n",
      "514\n",
      "513\n",
      "515\n",
      "514\n",
      "518\n",
      "516\n",
      "516\n",
      "513\n",
      "519\n",
      "516\n",
      "513\n",
      "513\n",
      "515\n",
      "523\n",
      "516\n",
      "521\n",
      "513\n",
      "516\n",
      "520\n",
      "514\n",
      "516\n",
      "521\n",
      "519\n",
      "513\n",
      "515\n",
      "515\n",
      "519\n",
      "515\n",
      "515\n",
      "513\n",
      "524\n",
      "517\n",
      "520\n",
      "518\n",
      "514\n",
      "520\n",
      "516\n",
      "520\n",
      "514\n",
      "519\n",
      "514\n",
      "521\n",
      "514\n",
      "522\n",
      "514\n",
      "519\n",
      "516\n",
      "516\n",
      "515\n",
      "514\n",
      "522\n",
      "517\n",
      "513\n",
      "518\n",
      "525\n",
      "514\n",
      "517\n",
      "516\n",
      "516\n",
      "529\n",
      "515\n",
      "516\n",
      "515\n",
      "514\n",
      "515\n",
      "514\n",
      "518\n",
      "516\n",
      "517\n",
      "515\n",
      "514\n",
      "526\n",
      "513\n",
      "522\n",
      "513\n",
      "521\n",
      "517\n",
      "524\n",
      "520\n",
      "516\n",
      "514\n",
      "518\n",
      "525\n",
      "522\n",
      "514\n",
      "513\n",
      "526\n",
      "522\n",
      "516\n",
      "519\n",
      "522\n",
      "516\n",
      "514\n",
      "522\n",
      "515\n",
      "519\n",
      "515\n",
      "527\n",
      "524\n",
      "515\n",
      "521\n",
      "518\n",
      "519\n",
      "525\n",
      "516\n",
      "522\n",
      "519\n",
      "514\n",
      "516\n",
      "531\n",
      "518\n",
      "515\n",
      "517\n",
      "514\n",
      "515\n",
      "518\n",
      "514\n",
      "516\n",
      "528\n",
      "515\n",
      "525\n",
      "524\n",
      "518\n",
      "516\n",
      "530\n",
      "519\n",
      "533\n",
      "519\n",
      "518\n",
      "518\n",
      "532\n",
      "526\n",
      "515\n",
      "523\n",
      "522\n",
      "529\n",
      "520\n",
      "516\n",
      "526\n",
      "533\n",
      "516\n",
      "517\n",
      "519\n",
      "518\n",
      "533\n",
      "520\n",
      "521\n",
      "519\n",
      "520\n",
      "516\n",
      "525\n",
      "533\n",
      "522\n",
      "519\n",
      "522\n",
      "514\n",
      "529\n",
      "518\n",
      "519\n",
      "519\n",
      "531\n",
      "517\n",
      "539\n",
      "518\n",
      "523\n",
      "516\n",
      "517\n",
      "524\n",
      "515\n",
      "538\n",
      "518\n",
      "514\n",
      "518\n",
      "516\n",
      "519\n",
      "523\n",
      "520\n",
      "519\n",
      "519\n",
      "515\n",
      "540\n",
      "522\n",
      "524\n",
      "518\n",
      "518\n",
      "523\n",
      "515\n",
      "516\n",
      "528\n",
      "513\n",
      "513\n",
      "518\n",
      "513\n",
      "514\n",
      "516\n",
      "516\n",
      "520\n",
      "519\n",
      "521\n",
      "514\n",
      "518\n",
      "516\n",
      "515\n",
      "519\n",
      "519\n",
      "514\n",
      "514\n",
      "514\n",
      "516\n",
      "519\n",
      "516\n",
      "514\n",
      "518\n",
      "519\n",
      "517\n",
      "518\n",
      "514\n",
      "514\n",
      "514\n",
      "519\n",
      "516\n",
      "517\n",
      "520\n",
      "516\n",
      "517\n",
      "513\n",
      "514\n",
      "516\n",
      "514\n",
      "514\n",
      "514\n",
      "517\n",
      "515\n",
      "516\n",
      "514\n",
      "514\n",
      "514\n",
      "513\n",
      "517\n",
      "520\n",
      "518\n",
      "515\n",
      "524\n",
      "516\n",
      "515\n",
      "525\n",
      "522\n",
      "515\n",
      "524\n",
      "519\n",
      "521\n",
      "517\n",
      "523\n",
      "525\n",
      "516\n",
      "530\n",
      "522\n",
      "519\n",
      "521\n",
      "517\n",
      "529\n",
      "528\n",
      "514\n",
      "530\n",
      "522\n",
      "523\n",
      "520\n",
      "518\n",
      "515\n",
      "527\n",
      "516\n",
      "528\n",
      "525\n",
      "518\n",
      "513\n",
      "527\n",
      "521\n",
      "518\n",
      "520\n",
      "528\n",
      "527\n",
      "513\n",
      "525\n",
      "519\n",
      "529\n",
      "526\n",
      "514\n",
      "528\n",
      "523\n",
      "524\n",
      "528\n",
      "525\n",
      "524\n",
      "527\n",
      "518\n",
      "532\n",
      "524\n",
      "543\n",
      "514\n",
      "524\n",
      "514\n",
      "514\n",
      "514\n",
      "520\n",
      "518\n",
      "517\n",
      "515\n",
      "514\n",
      "527\n",
      "525\n",
      "518\n",
      "519\n",
      "524\n",
      "526\n",
      "526\n",
      "514\n",
      "517\n",
      "527\n",
      "529\n",
      "525\n",
      "530\n",
      "524\n",
      "527\n",
      "517\n",
      "525\n",
      "530\n",
      "525\n",
      "522\n",
      "544\n",
      "526\n",
      "514\n",
      "518\n",
      "515\n",
      "513\n",
      "515\n",
      "523\n",
      "515\n",
      "514\n",
      "519\n",
      "525\n",
      "518\n",
      "517\n",
      "517\n",
      "517\n",
      "519\n",
      "523\n",
      "524\n",
      "516\n",
      "518\n",
      "516\n",
      "516\n",
      "514\n",
      "518\n",
      "524\n",
      "523\n",
      "516\n",
      "515\n",
      "524\n",
      "519\n",
      "517\n",
      "525\n",
      "519\n",
      "515\n",
      "524\n",
      "517\n"
     ]
    }
   ],
   "source": [
    "for doc in data:\n",
    "    for seg in doc:\n",
    "        length = len(tokenizer.tokenize(seg,add_special_tokens=True))\n",
    "        if length > 512:\n",
    "            print(length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"facebook/bart-large\")\n",
    "model.knowledge_encoder = AutoModel.from_pretrained(\"facebook/bart-large\").encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   894,  1437,     2,     2,    38,   524,    45,   228, 12997,\n",
       "          3894,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('He </s></s> I am not perfrct', add_special_tokens=True, padding='max_length', truncation=True, max_length=512, return_tensors='pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e0777d1a0df2a4bd0f3d146dc1b64bcae571d346f8053e0a194375fb9e0aaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

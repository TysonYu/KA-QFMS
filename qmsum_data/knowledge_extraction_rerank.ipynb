{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import jsonlines\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "def file_reader(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        src = []\n",
    "        tgt = []\n",
    "        for obj in reader:\n",
    "            src.append(obj['src'])\n",
    "            tgt.append(obj['tgt'])\n",
    "    return src, tgt\n",
    "\n",
    "def get_query(input):\n",
    "    query = []\n",
    "    src = []\n",
    "    for item in input:\n",
    "        temp = item.split('</s>')\n",
    "        query.append(temp[0].replace('<s>','').strip())\n",
    "        src.append(temp[1].replace('<s>','').strip())\n",
    "    return query, src\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input['input_ids'] = encoded_input['input_ids'].cuda()\n",
    "    encoded_input['token_type_ids'] = encoded_input['token_type_ids'].cuda()\n",
    "    encoded_input['attention_mask'] = encoded_input['attention_mask'].cuda()\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# segment 文章到paragraph中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells requires jupyter and notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "src, tgt = file_reader('processed_data_turn_split/train.jsonl')\n",
    "querys, src = get_query(src)\n",
    "max_input_len = 512\n",
    "number_of_segment = 16\n",
    "src = [i.split('<turn_seperator>') for i in src]\n",
    "\n",
    "segmented_input = []\n",
    "non_segmented_input = []\n",
    "\n",
    "for i in tqdm(range(len(src))):\n",
    "    seg_counter = 0\n",
    "    doc = src[i]\n",
    "    query = querys[i]\n",
    "    this_doc = []\n",
    "    this_seg = query + '</s></s>'\n",
    "    counter = len(tokenizer.tokenize(this_seg,add_special_tokens=False))\n",
    "    for sent in doc:\n",
    "        length = len(tokenizer.tokenize(sent,add_special_tokens=False))\n",
    "        if counter + length < max_input_len:\n",
    "            this_seg = this_seg + ' ' + sent\n",
    "            counter += length\n",
    "        else:\n",
    "            speaker = sent.split(':')[0]\n",
    "            for subsent in sent_tokenize(sent):\n",
    "                length = len(tokenizer.tokenize(subsent,add_special_tokens=False))\n",
    "                if counter + length < max_input_len:\n",
    "                    this_seg = this_seg + ' ' + subsent\n",
    "                    counter += length\n",
    "                else:\n",
    "                    # print(subsent)\n",
    "                    if speaker not in subsent:\n",
    "                        subsent = speaker + ': ' + subsent\n",
    "                    this_doc.append(this_seg.split('</s></s>')[-1])\n",
    "                    this_seg = query + '</s></s>'+ subsent\n",
    "                    counter = len(tokenizer.tokenize(this_seg, add_special_tokens=False))\n",
    "    this_doc.append(this_seg.split('</s></s>')[-1])\n",
    "    non_segmented_input.append([query + '</s></s>' + doc for doc in this_doc])\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open( 'processed_data_no_sent_split/test_sort_16.pkl', 'wb') as f:\n",
    "#     pickle.dump(segmented_input, f)\n",
    "with open('processed_data_turn_split/train_all_seg.pkl', 'wb') as f:\n",
    "    pickle.dump(non_segmented_input, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 只取前16段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data_turn_split/val_all_seg.pkl', 'rb') as f:\n",
    "    segmented_input = pickle.load(f)\n",
    "\n",
    "number_of_segment = 4\n",
    "new_segmented_input = []\n",
    "\n",
    "for doc in segmented_input:\n",
    "    if len(doc) < number_of_segment:\n",
    "        new_segmented_input.append(doc + ['' for i in range(number_of_segment - len(doc))])\n",
    "    else:\n",
    "        new_segmented_input.append(doc[:number_of_segment])\n",
    "\n",
    "with open('processed_data_turn_split/val_4_no_sort.pkl', 'wb') as f:\n",
    "    pickle.dump(new_segmented_input, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 只用knowledge，并且重新排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openie import StanfordOpenIE\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "properties = {\n",
    "    'openie.affinity_probability_cap': 2 / 3,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( 'processed_data_turn_split/train_all_seg.pkl', 'rb') as f:\n",
    "    segmented_input = pickle.load(f)\n",
    "\n",
    "with StanfordOpenIE(properties=properties) as client:\n",
    "    new_segmented_input = []\n",
    "    for number, one_segmented_input in tqdm(enumerate(segmented_input)):\n",
    "        query_without_stopwords = \" \".join([item for item in one_segmented_input[0].split('</s></s>')[0].split() if item not in stopwords.words()])\n",
    "        print(query_without_stopwords)\n",
    "        print(len(one_segmented_input))\n",
    "        this_doc_dic = {}\n",
    "        for i,item in enumerate(segmented_input[number]):\n",
    "            origin_item = copy.deepcopy(item)\n",
    "            item = item.split('</s></s>')[-1]\n",
    "            counter = 0\n",
    "            triples = []\n",
    "            for triple in client.annotate(item):\n",
    "                for word in triple['subject'].split():\n",
    "                    if word in query_without_stopwords and word not in stopwords.words():\n",
    "                        counter += 1\n",
    "                        print(triple, query_without_stopwords)\n",
    "                        triples.append(triple)\n",
    "                for word in triple['object'].split():\n",
    "                    if word in query_without_stopwords and word not in stopwords.words():\n",
    "                        counter += 1\n",
    "                        triples.append(triple)\n",
    "                        print(triple, query_without_stopwords)\n",
    "            this_doc_dic[origin_item] = [triples, counter]\n",
    "        new_segmented_input.append(this_doc_dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single rerank 处理重新排序的结果 (rerank) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple2text(triples):\n",
    "    output = \"\"\n",
    "    for triple in triples:\n",
    "        this_triple_text = 'subject: ' + triple['subject'] + ' relation: ' + triple['relation'] +  ' object: ' + triple['object'] + '</s>'\n",
    "        output += this_triple_text\n",
    "    return output\n",
    "    \n",
    "with open( 'processed_data_turn_split/test_all_seg_with_triple.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "new_seg_input = []\n",
    "triples = []\n",
    "number_of_segment = 16\n",
    "for doc in data:\n",
    "    new = {k: v for k, v in sorted(doc.items(), key=lambda item: item[1][1], reverse=True)}\n",
    "    new_seg_list = list(new.keys())\n",
    "    # print(new[new_seg_list[0]][0])\n",
    "    # print(triple2text(new[new_seg_list[0]][0]))\n",
    "    # break\n",
    "    if len(new_seg_list) < number_of_segment:\n",
    "        new_seg_input.append(new_seg_list + ['' for i in range(number_of_segment - len(new_seg_list))])\n",
    "        triples.append([triple2text(new[new_seg_list[i]][0]) for i in range(len(new_seg_list))] + ['' for i in range(number_of_segment - len(new_seg_list))])\n",
    "    else:\n",
    "        new_seg_input.append(new_seg_list[:number_of_segment])\n",
    "        triples.append([triple2text(new[new_seg_list[i]][0]) for i in range(number_of_segment)])\n",
    "\n",
    "# with open('processed_data_turn_split/test_16_rerank.pkl', 'wb') as f:\n",
    "#     pickle.dump(new_seg_input, f)\n",
    "\n",
    "# with open('processed_data_turn_split/test_triple.pkl', 'wb') as f:\n",
    "#     pickle.dump(triples, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duals rerank 处理重新排序的结果 (QA + Knowledge) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 179/1257 [08:06<23:27,  1.31s/it]  /home/tiezheng/.conda/envs/3090/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in true_divide\n",
      " 34%|███▍      | 430/1257 [17:20<33:20,  2.42s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-fe52f5263af6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mnew_seg_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_seg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumber_of_segment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtriples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtriple2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_seg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_data_turn_split/train_4_rerank.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-fe52f5263af6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mnew_seg_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_seg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumber_of_segment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtriples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtriple2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_seg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_data_turn_split/train_4_rerank.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-fe52f5263af6>\u001b[0m in \u001b[0;36mtriple2text\u001b[0;34m(triples)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                         \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3090/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     19\u001b[0m         return [\n\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         ]\n",
      "\u001b[0;32m~/.conda/envs/3090/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         ]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "out_words = {}\n",
    "for word in stopwords.words():\n",
    "    out_words[word] = 1\n",
    "\n",
    "\n",
    "def triple2text(triples):\n",
    "    words = []\n",
    "    for triple in triples:\n",
    "        for item in triple['subject'].split():\n",
    "            if item not in words:\n",
    "                if item.isalpha():\n",
    "                    if item not in stopwords.words():\n",
    "                        words.append(item)\n",
    "    for triple in triples:\n",
    "        for item in triple['relation'].split():\n",
    "            if item not in words:\n",
    "                if item.isalpha():\n",
    "                    if item not in stopwords.words():\n",
    "                        words.append(item)\n",
    "    for triple in triples:\n",
    "        for item in triple['object'].split():\n",
    "            if item not in words:\n",
    "                if item.isalpha():\n",
    "                    if item not in stopwords.words():\n",
    "                        words.append(item)\n",
    "    output = \"</s>\".join(words)\n",
    "        # this_triple_text = triple['subject'] + '</s>' + triple['relation'] +  '</s>' + triple['object'] + '</s>'\n",
    "        # output += this_triple_text\n",
    "    return output\n",
    "    \n",
    "with open('processed_data_turn_split/train_all_seg_with_triple.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "new_seg_input = []\n",
    "triples = []\n",
    "number_of_segment = 4\n",
    "\n",
    "for doc in tqdm(data):\n",
    "    keys = list(doc.keys())\n",
    "    query = keys[0].split('</s></s>')[0]\n",
    "    this_doc = [item.split('</s></s>')[-1] for item in keys]\n",
    "    \n",
    "    #Encode query and docs\n",
    "    query_emb = encode(query)\n",
    "    doc_emb = encode(this_doc)\n",
    "    #Compute dot score between query and all document embeddings\n",
    "    scores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n",
    "    knowledge_score = []\n",
    "    for key in keys:\n",
    "        knowledge_score.append(doc[key][1])\n",
    "    knowledge_score = knowledge_score / np.linalg.norm(knowledge_score)\n",
    "    merge_score = []\n",
    "    for i in range(len(knowledge_score)):\n",
    "        merge_score.append(knowledge_score[i] + scores[i])\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        doc[keys[i]].append(merge_score[i])\n",
    "\n",
    "    new = {k: v for k, v in sorted(doc.items(), key=lambda item: item[1][-1], reverse=True)}\n",
    "    new_seg_list = list(new.keys())\n",
    "    \n",
    "    if len(new_seg_list) < number_of_segment:\n",
    "        new_seg_input.append(new_seg_list + ['' for i in range(number_of_segment - len(new_seg_list))])\n",
    "        triples.append([triple2text(new[new_seg_list[i]][0]) for i in range(len(new_seg_list))] + ['' for i in range(number_of_segment - len(new_seg_list))])\n",
    "    else:\n",
    "        new_seg_input.append(new_seg_list[:number_of_segment])\n",
    "        triples.append([triple2text(new[new_seg_list[i]][0]) for i in range(number_of_segment)])\n",
    "\n",
    "with open('processed_data_turn_split/train_4_rerank.pkl', 'wb') as f:\n",
    "    pickle.dump(new_seg_input, f)\n",
    "\n",
    "with open('processed_data_turn_split/train_4_triple.pkl', 'wb') as f:\n",
    "    pickle.dump(triples, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从两个分数（QA和knowledge）中得到最终的排序结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "out_words = {}\n",
    "for word in stopwords.words():\n",
    "    out_words[word] = 1\n",
    "\n",
    "def triple2text(triples):\n",
    "    words = []\n",
    "    for triple in triples:\n",
    "        for item in triple['subject'].split():\n",
    "            if item not in words:\n",
    "                if item.isalpha():\n",
    "                    if item not in stopwords.words():\n",
    "                        words.append(item)\n",
    "    for triple in triples:\n",
    "        for item in triple['relation'].split():\n",
    "            if item not in words:\n",
    "                if item.isalpha():\n",
    "                    if item not in stopwords.words():\n",
    "                        words.append(item)\n",
    "    for triple in triples:\n",
    "        for item in triple['object'].split():\n",
    "            if item not in words:\n",
    "                if item.isalpha():\n",
    "                    if item not in stopwords.words():\n",
    "                        words.append(item)\n",
    "    output = \"</s>\".join(words)\n",
    "        # this_triple_text = triple['subject'] + '</s>' + triple['relation'] +  '</s>' + triple['object'] + '</s>'\n",
    "        # output += this_triple_text\n",
    "    return output\n",
    "\n",
    "with open('processed_data_turn_split/test_all_seg_with_triple.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "with open('processed_data_turn_split/test_rankscores.pkl', 'rb') as f:\n",
    "    qa_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/281 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39629098796272816, 0.0, 0.5004784694158335, 0.4339330207563356, 0.8503727076038221, 0.5425439163487842, 0.7586785438731851, 0.7870905196367776, 0.5776469966170464, 0.4896485936732833, 0.41190044296728734, 0.2466511403305259, 0.34279931585563905, 0.6003288209300695, 0.6032938205793645, 0.817669441731727, 0.1701137659773067, 0.4440076183898867, 0.6683264685093961, 0.3689582337421294, 0.44705734834710076, 0.5836230515644137, 0.2612245968242321, 0.20285591962922397, 0.7000073162441122, 0.4353438830931291, 0.36996960197689666, 0.3161959846984747, 2.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(data))):\n",
    "    keys = list(data[i].keys())\n",
    "    a = qa_scores[i][0]\n",
    "    this_qascores = [(float(i)-min(a))/(max(a)-min(a)) for i in a]\n",
    "    ka_scores = []\n",
    "    for key in keys:\n",
    "        ka_scores.append(data[i][key][-1])\n",
    "    a = ka_scores\n",
    "    if max(a)-min(a) == 0:\n",
    "        pass\n",
    "    else:    \n",
    "        ka_scores = [(float(i)-min(a))/(max(a)-min(a)) for i in a]\n",
    "    all_scores = []\n",
    "    for j in range(len(ka_scores)):\n",
    "        all_scores.append(ka_scores[j]+this_qascores[j])\n",
    "    print(all_scores)\n",
    "    print(ka_scores)\n",
    "    break\n",
    "    for j in range(len(keys)):\n",
    "        this_seg_info = {}\n",
    "        this_seg_info['triples'] = data[i][keys[j]]\n",
    "        this_seg_info['qa_score'] = this_qascores[j]\n",
    "        this_seg_info['ka_score'] = ka_scores[j]\n",
    "        this_seg_info['all_scores'] = all_scores[j]\n",
    "        # key_phrase = triple2text(data[i][keys[j]][0])\n",
    "        # this_seg_info['key_phrase'] = key_phrase\n",
    "        data[i][keys[j]] = this_seg_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39629098796272816, 0.0, 0.5004784694158335, 0.4339330207563356, 0.8503727076038221, 0.5425439163487842, 0.7586785438731851, 0.7870905196367776, 0.5776469966170464, 0.4896485936732833, 0.41190044296728734, 0.2466511403305259, 0.34279931585563905, 0.6003288209300695, 0.6032938205793645, 0.817669441731727, 0.1701137659773067, 0.4440076183898867, 0.6683264685093961, 0.3689582337421294, 0.44705734834710076, 0.5836230515644137, 0.2612245968242321, 0.20285591962922397, 0.7000073162441122, 0.2535257012749473, 0.36996960197689666, 0.3161959846984747, 1.0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = []\n",
    "for doc in data:\n",
    "    keywords = []\n",
    "    for seg_results in doc:\n",
    "        keywords += seg_results.split('</s>')\n",
    "    keywords = list(set(keywords))\n",
    "    keywords = [item for item in keywords if len(item) > 2]\n",
    "    all_keywords.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'processed_data_turn_split/test_rankscores.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-796e8369cb00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_data_turn_split/test_rankscores.jsonl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mquerys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-f7b202779942>\u001b[0m in \u001b[0;36mfile_reader\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'7'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mjsonlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/3090/lib/python3.6/site-packages/jsonlines/jsonlines.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, mode, loads, dumps, compact, sort_keys, flush)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReader\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8-sig\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m     kwargs = dict(\n\u001b[1;32m    625\u001b[0m         \u001b[0mloads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'processed_data_turn_split/test_rankscores.jsonl'"
     ]
    }
   ],
   "source": [
    "src, tgt = file_reader('processed_data_turn_split/train.jsonl')\n",
    "querys, src = get_query(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14826708439396907\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "overlap = 0\n",
    "for i in range(len(all_keywords)):\n",
    "    counter += len(all_keywords[i])\n",
    "    for word in all_keywords[i]:\n",
    "        if word in tgt[i]:\n",
    "            overlap += 1\n",
    "print(overlap/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data_turn_split/test_8_dual_rank.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"summarize the whole meeting .</s></s>barry hughes: it 's a potential barrier , but i do n't think it is a barrier . there was a shortage of registered intermediaries in wales , and i know that the ministry of justice have taken action to deal with that , and we have had a number of people who are now in a position to act as intermediaries . now , of course , if they were to decide not to do that anymore , we may have a problem , but , in turn , we would be looking to recruit more people into those positions . so , yes , it has the potential to serve as a barrier , but in practice , i do n't think it would be a barrier . i think , particularly given the very low numbers we 're talking about , we would be able to manage it . i 've got no significant concerns , i have to say . lynne neagle am: thank you . well , we 've come to the end of our time . can i thank you for attending , the three of you , and for your answers , which have been fascinating and very clear and most helpful to the committee ? you will be sent a transcript following the meeting to check for accuracy . thank you again for your time in coming here today . diolch yn fawr .  barry hughes: thank you . it 's been a pleasure .  lynne neagle am: item 3 , then , is papers to note . paper to note 1 is a letter from me to the business committee requesting an extension on the deadline for the bill , which has now been agreed . paper to note 2 is a letter from myself to the deputy minister for health and social services requesting some further information from cafcass cymru on the bill . paper to note 3 is a letter from myself to the minister for education regarding diamond reform implementation ahead of our scrutiny session on 4 july . are members happy to note those ? item 4 , then . can i propose in accordance with standing order 17.42 that the committee resolves to meet in private for the remainder of the meeting ? are members content ? thank you .\", \"summarize the whole meeting .</s></s>barry hughes: so , you are probably aware , but forgive me if i just explain quickly anyway , when we approach a file of material evidence submitted by the police we apply the code for crown prosecutors , which has a two-stage test . the first stage is whether there 's sufficient evidence to provide a realistic prospect of conviction , and the second stage is— . and you only get to the second stage if the first stage is satisfied . if there is n't enough evidence , we do n't go on to consider whether it 's in the public interest , because we would n't put an offence before the courts if we did n't think there was a realistic prospect of conviction . so , we only get on to the public interest stage once the evidential stage is satisfied . so , to return to the point , if we have an offence where , let 's say , there is a light smack at the time , the police apply the same code before they bring a case to us . we do n't always agree with the police ; generally we do , but we do n't always agree . it 's a matter for them whether they refer a matter to the crown prosecution service . so , if a police officer takes witness statements in relation to that case—the light smacking on the leg—at present it 's unlikely that would come to the cps , because they would look at it and say , 'reasonable chastisement provides for a defence . ' if that defence is removed , then obviously there is a greater possibility that it would be referred to the cps . i would like to think—and i think this is what will probably happen in practice—that the police would take a view that the evidential test may have been satisfied because the defence had been removed , but it would n't be in the public interest to prosecute . it may be that the police decide that it is—it may have been two smacks , three smacks , so it moves towards the end of the spectrum that would suggest that matters are becoming rather more serious . so , it may be referred to the cps for a charging decision . we would then apply independently the same test , and we would probably conclude that the evidential stage was met in that instance because the defence no longer exists , which takes us on to considering the public interest .\"]\n"
     ]
    }
   ],
   "source": [
    "print(data[0][:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2 (default, Jan 26 2021, 13:30:48) \n[GCC 5.4.0 20160609]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
